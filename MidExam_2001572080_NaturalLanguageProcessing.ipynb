{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MidExam_2001572080_NaturalLanguageProcessing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0duP-kZc3qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mike Christ Heru - 2001572080 - LA05\n",
        "\n",
        "# [15%] Write a simple python code (in a Python notebook please) implementing:\n",
        "# an unsmoothed unigram, bigram, and trigram language model. \n",
        "# Your code should be able to estimate the probability of a given sentence and generate a new sentence\n",
        "\n",
        "\n",
        "import math\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize as st\n",
        "from nltk.tokenize import word_tokenize as wt\n",
        "\n",
        "unigram_dict = {}\n",
        "bigram_dict = {}\n",
        "trigram_dict = {}\n",
        "SENTENCE_START = \"<s>\"\n",
        "SENTENCE_END = \"</s>\"\n",
        "unigram_corpus_length = 0\n",
        "bigram_corpus_length = 0\n",
        "trigram_corpus_length = 0\n",
        "unigram_unique_words = 0\n",
        "bigram_unique_words = 0\n",
        "trigram_unique_words = 0\n",
        "bigram_unique_set = set()\n",
        "trigram_unique_set = set()\n",
        "\n",
        "def calcgram(s):\n",
        "\treturn len(s) - 2\n",
        "\n",
        "def calculate_unigram(s):\n",
        "\tres = 0\n",
        "\tfor sentence in s:\n",
        "\t\tres += len(sentence) - 2\n",
        "\treturn res\n",
        "\n",
        "def calculate_bigram(s):\n",
        "\tres = 0\n",
        "\tfor sentence in s:\n",
        "\t\tres += len(sentence) - 1\n",
        "\treturn res\n",
        "\n",
        "def calculate_trigram(s):\n",
        "\tres = 0\n",
        "\tfor sentence in s:\n",
        "\t\tres += len(sentence) - 2\n",
        "\treturn res\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "  new_words = [SENTENCE_START]\n",
        "  for word in wt(s):\n",
        "      new_words.append(word)\n",
        "  new_words.append(SENTENCE_END)\n",
        "  return new_words\n",
        "\n",
        "def preprocess_sentences(s):\n",
        "  sentences = st(s)\n",
        "  new_sentences = []\n",
        "  for sentence in sentences:\n",
        "    new_sentences.append(preprocess_sentence(sentence[:-1])[:])\n",
        "  return new_sentences\n",
        "\n",
        "def preprocess(s):\n",
        "  unigram(s)\n",
        "  bigram(s)\n",
        "  trigram(s)\n",
        "\n",
        "def unigram(s):\n",
        "  global unigram_corpus_length\n",
        "  for sentence in s:\n",
        "    for word in sentence:\n",
        "      try:\n",
        "        unigram_dict[word] += 1\n",
        "      except:\n",
        "        unigram_dict[word] = 1\n",
        "      if(word != SENTENCE_START and word != SENTENCE_END):\n",
        "        unigram_corpus_length += 1\n",
        "  unigram_unique_words = len(unigram_dict) - 2\n",
        "\n",
        "\n",
        "def bigram(s):\n",
        "  global bigram_corpus_length\n",
        "  for sentence in s:\n",
        "    previous_word = None\n",
        "    for word in sentence:\n",
        "      if(previous_word != None):\n",
        "        try:\n",
        "          bigram_dict[(previous_word,word)] += 1\n",
        "        except:\n",
        "          bigram_dict[(previous_word,word)] = 1\n",
        "        if previous_word != SENTENCE_START and word != SENTENCE_END:\n",
        "          bigram_unique_set.add((previous_word, word))\n",
        "      previous_word = word\n",
        "  bigram_unique_words = len(bigram_dict)\n",
        "\n",
        "def trigram(s):\n",
        "  global trigram_corpus_length\n",
        "  for sentence in s:\n",
        "    first_word = None\n",
        "    second_word = None\n",
        "    for word in sentence:\n",
        "      if(first_word != None and second_word != None):\n",
        "        try:\n",
        "          trigram_dict[(first_word,second_word,word)] += 1\n",
        "        except:\n",
        "          trigram_dict[(first_word,second_word,word)] = 1\n",
        "        if first_word != SENTENCE_START and word != SENTENCE_END:\n",
        "          trigram_unique_set.add((first_word,second_word,word))\n",
        "      first_word = second_word\n",
        "      second_word = word\n",
        "  trigram_unique_words = len(trigram_dict)\n",
        "\n",
        "def probability_unigram(word,smooth):\n",
        "\ttry:\n",
        "\t\tresult_numerator = unigram_dict[word]\n",
        "\texcept:\n",
        "\t\tresult_numerator = 0\n",
        "\ttry:\n",
        "\t\tresult_denumerator = unigram_unique_words\n",
        "\texcept:\n",
        "\t\tresult_denumerator = 0\n",
        "\tif(smooth):\n",
        "\t\tresult_numerator += 1\n",
        "\t\tresult_denumerator += 1\n",
        "\treturn float(result_numerator) / float(result_denumerator)\n",
        "\n",
        "def probability_bigram(previous_word,word,smooth):\n",
        "\ttry:\n",
        "\t\tresult_numerator = bigram_dict[(previous_word,word)]\n",
        "\texcept:\n",
        "\t\tresult_numerator = 0\n",
        "\ttry:\n",
        "\t\tresult_denumerator = unigram_dict[previous_word]\n",
        "\texcept:\n",
        "\t\tresult_denumerator = 0\n",
        "\tif(smooth):\n",
        "\t\tresult_numerator += 1\n",
        "\t\tresult_denumerator += 1\n",
        "\tif(result_numerator == 0 or result_denumerator ==0):\n",
        "\t\treturn 0.0\n",
        "\treturn float(result_numerator) / float(result_denumerator)\n",
        "\n",
        "def probability_trigram(first_word,second_word,word,smooth):\n",
        "\ttry:\n",
        "\t\tresult_numerator = trigram_dict[(first_word,second_word,word)]\n",
        "\texcept:\n",
        "\t\tresult_numerator = 0\n",
        "\ttry:\n",
        "\t\tresult_denumerator = bigram_dict[(first_word,second_word)]\n",
        "\texcept:\n",
        "\t\tresult_denumerator = 0\n",
        "\tif(smooth):\n",
        "\t\tresult_numerator += 1\n",
        "\t\tresult_denumerator += 1\n",
        "\tif(result_numerator == 0 or result_denumerator ==0):\n",
        "\t\treturn 0.0\n",
        "\treturn float(result_numerator) / float(result_denumerator)\n",
        "\n",
        "def sentence_probability(gram_type,sentence,smooth):\n",
        "\tresult = 0.0\n",
        "\tif(gram_type == \"Unigram\"):\n",
        "\t\tfor word in sentence :\n",
        "\t\t\tif(word != SENTENCE_START and word != SENTENCE_END):\n",
        "\t\t\t\tif(result == 0.0):\n",
        "\t\t\t\t\tresult = probability_unigram(word,smooth)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tresult *= probability_unigram(word,smooth)\n",
        "\n",
        "\telif(gram_type == \"Bigram\"):\n",
        "\t\tprevious_word = None\n",
        "\t\tfor word in sentence:\n",
        "\t\t\tif(previous_word != None  and word != SENTENCE_END):\n",
        "\t\t\t\tif(result == 0.0):\n",
        "\t\t\t\t\tresult = probability_bigram(previous_word,word,smooth)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tresult *= probability_bigram(previous_word,word,smooth)\n",
        "\t\t\tprevious_word = word\n",
        "\n",
        "\telif(gram_type ==\"Trigram\"):\n",
        "\t\tfirst_word = None\n",
        "\t\tsecond_word = None\n",
        "\t\tfor word in sentence:\n",
        "\t\t\tif(first_word != None and word != SENTENCE_END):\n",
        "\t\t\t\tif(result == 0.0):\n",
        "\t\t\t\t\tresult = probability_trigram(first_word,second_word,word,smooth)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tresult *= probability_trigram(first_word,second_word,word,smooth)\n",
        "\t\t\tprevious_word = word\n",
        "\treturn result\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPQ5H5ULf1Sd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtcnnne0g5Or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ab9ac33-c359-43e7-e19a-2afefaa218e9"
      },
      "source": [
        "train_sentences = \"She is Beautiful. She is Wonderful. He is Greek.\"\n",
        "sentences = preprocess_sentences(train_sentences)\n",
        "preprocess(sentences)\n",
        "input_sentences = preprocess_sentence(\"She is Wonderful\")\n",
        "sentence_probability(\"Bigram\",input_sentences,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2222222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0IR-TtUP8d1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abbc7ddb-48e9-403d-e3d9-c59d558436d6"
      },
      "source": [
        "# [5%] Add an option to your program to compute the perplexity of a test set\n",
        "\n",
        "def calculate_perplexity(gram_type,s,smooth):\n",
        "\tresult = 0.0\n",
        "\tif(gram_type == \"Unigram\"):\n",
        "\t\tnumber_of_unigram = calcgram(s)\n",
        "\t\ttemp = sentence_probability(\"Unigram\",s,smooth)\n",
        "\t\tif(temp != 0.0):\n",
        "\t\t\treturn math.pow(temp,-1 / number_of_unigram)\n",
        "\t\telse:\n",
        "\t\t\treturn 0.0\n",
        "\t\t\n",
        "\n",
        "\telif(gram_type == \"Bigram\"):\n",
        "\t\tnumber_of_bigram = calcgram(s)\n",
        "\t\ttemp = sentence_probability(\"Bigram\",s,smooth)\n",
        "\t\tif(temp != 0.0):\n",
        "\t\t\treturn math.pow(temp,-1 / number_of_bigram)\n",
        "\t\telse:\n",
        "\t\t\treturn 0.0\t\t\n",
        "\n",
        "\telif(gram_type ==\"Trigram\"):\n",
        "\t\tnumber_of_trigram = calcgram(s)\n",
        "\t\ttemp = sentence_probability(\"Trigram\",s,smooth)\n",
        "\t\tif(temp != 0.0):\n",
        "\t\t\treturn math.pow(temp,-1 / number_of_trigram)\n",
        "\t\telse:\n",
        "\t\t\treturn 0.0\n",
        "\t\t\n",
        "\treturn result\n",
        "\n",
        "calculate_perplexity(\"Bigram\",input_sentences,False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6509636244473134"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flWvldRGQwp1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5d33d55-17e7-4f58-bad3-28d13da9634b"
      },
      "source": [
        "# [10%] Add an option to your program to do smoothing or discounting\n",
        "\n",
        "# Answer : To do smoothing, just put the False value into True (since the last parameter is for smoothing)\n",
        "# Here in my source code, I only apply the add-1 smoothing (using this reference https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
        "\n",
        "input_sentences = preprocess_sentence(\"She is Gorgeous\")\n",
        "# sentence_probability(\"Bigram\",input_sentences,False) # will get 0.0 as the result\n",
        "sentence_probability(\"Bigram\",input_sentences,True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u31EtSqRoHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7b3428aa-7ce3-4d36-f1b1-62912c4f6347"
      },
      "source": [
        "# [5%] Calculate the probability and the perplexity of a test set\n",
        "\n",
        "newTrainSet = \"Hope is the thing with feathers. That perches in the soul, And sings the tune without the words, And never stops at all, And sweetest in the gale is heard; And sore must be the storm. That could abash the little bird. That kept so many warm. Iâ€™ve heard it in the chillest land, And on the strangest sea; Yet, never, in extremity,It asked a crumb of me.\"\n",
        "sentences = preprocess_sentences(newTrainSet)\n",
        "preprocess(sentences)\n",
        "input_sentences = preprocess_sentence(\"Hope is the thing with soul\")\n",
        "print(\"Probability without Smoothing = \", str(sentence_probability(\"Bigram\",input_sentences,False)))\n",
        "print(\"Probability with Smoothing = \", str(sentence_probability(\"Bigram\",input_sentences,True)))\n",
        "print(\"Perplexity without Smoothing = \", str(calculate_perplexity(\"Bigram\",input_sentences,False)))\n",
        "print(\"Perplexity with Smoothing = \", str(calculate_perplexity(\"Bigram\",input_sentences,True)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability without Smoothing =  0.0\n",
            "Probability with Smoothing =  0.007407407407407408\n",
            "Perplexity without Smoothing =  0.0\n",
            "Perplexity with Smoothing =  2.2649344008227015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1P7-Dl1Tv5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "12170ae1-f23b-4567-a387-6e204953d167"
      },
      "source": [
        "# [5%] Compare the smoothed bigram and the smoothed trigram model of a test set.\n",
        "\n",
        "print(\"Probability of Smoothed Bigram = \", str(sentence_probability(\"Bigram\",input_sentences,True)))\n",
        "print(\"Probability of Smoothed Trigram = \", str(sentence_probability(\"Trigram\",input_sentences,True)))\n",
        "\n",
        "# The Smoothed Bigram probability is greater than the Smoothed Trigram Probability."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of Smoothed Bigram =  0.007407407407407408\n",
            "Probability of Smoothed Trigram =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}